{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:90: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.332074\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.344449\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.294566\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.320626\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.300294\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.300256\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.275375\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.273973\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.278428\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.271183\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.247332\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.208678\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.223641\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.146708\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.122668\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.993432\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.875436\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.754396\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.376826\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.243472\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.108507\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.985318\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.834108\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.693762\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.750117\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.700807\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.784419\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.628341\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.565306\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.438517\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.892467\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.574269\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.621533\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.631867\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.568378\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.350402\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.668208\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.578149\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.573596\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.647248\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.625190\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.591350\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.270846\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.399485\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.608939\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.506157\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.409508\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.362512\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.501408\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.426499\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.537759\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.632629\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.351630\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.514758\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.347529\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.493388\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.547520\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.386710\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.332488\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.346447\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.497566\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.554826\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.399991\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.431477\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.316623\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.404821\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.441335\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.471564\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.531079\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.294019\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.412550\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.522957\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.418253\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.442523\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.391608\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.332738\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.427296\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.350870\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.397070\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.259270\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.280779\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.372217\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.338222\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.243404\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.178729\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.522395\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.493164\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.373372\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.526931\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.307481\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.378607\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.230299\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.339650\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.439448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:153: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "D:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3250, Accuracy: 9028/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.455013\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.238540\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.296613\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.308804\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.285329\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.401596\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.422094\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.343862\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.434531\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.325801\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.177775\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.377713\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.416484\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.388439\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.629142\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.221194\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.349990\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.375740\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.297942\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.277946\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.126216\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.220779\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.383581\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.501889\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.298826\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.374424\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.511061\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.353448\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.308471\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.200022\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.336528\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.267556\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.541825\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.276962\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.220522\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.356645\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.317111\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.392302\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.161319\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.240007\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.348592\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.384845\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.277501\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.188860\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.227928\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.144386\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.260393\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.373848\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.328343\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.453686\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.317573\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.314799\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.160292\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.162071\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.413867\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.252569\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.274240\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.260395\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.322043\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.388756\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.570348\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.312769\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.144807\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.269625\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.340578\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.371845\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.210930\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.464994\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.518708\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.259130\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.220535\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.453497\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.212072\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.225133\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.247268\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.269179\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.343044\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.464096\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.184654\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.181565\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.139449\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.236809\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.150594\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.469795\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.361263\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.392965\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.295991\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.189264\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.154124\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.343239\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.300111\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.221694\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.268888\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.198503\n",
      "\n",
      "Test set: Average loss: 0.2534, Accuracy: 9231/10000 (92%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.334015\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.556474\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.351178\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.374633\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.318497\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.367051\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.131981\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.125594\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.326765\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.396392\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.154830\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.316005\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.372032\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.590092\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.240625\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.349566\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.243064\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.459048\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.514917\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.331855\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.198856\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.393453\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.335538\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.382363\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.280220\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.352154\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.148341\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.379995\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.185313\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.221525\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.449125\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.178469\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.328516\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.263592\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.156847\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.322333\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.123029\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.251996\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.165833\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.255949\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.101208\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.152485\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.299712\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.244623\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.453784\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.234690\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.443708\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.183907\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.172556\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.158059\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.280501\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.155327\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.159842\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.230061\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.123435\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.218972\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.226948\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.332576\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.252196\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.267583\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.268964\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.331132\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.247479\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.198583\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.255081\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.195486\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.230345\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.347863\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.277411\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.264935\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.100013\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.184482\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.170362\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.098075\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.211782\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.314452\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.239281\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.275790\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.102418\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.288428\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.179866\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.045534\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.084799\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.256750\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.101498\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.149764\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.144494\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.271913\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.186315\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.127170\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.084421\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.266297\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.135916\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.234860\n",
      "\n",
      "Test set: Average loss: 0.2060, Accuracy: 9373/10000 (93%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.232964\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.256516\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.112688\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.196989\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.097709\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.276457\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.140250\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.226691\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.094916\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.114505\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.100898\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.325918\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.098362\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.177071\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.144872\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.242002\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.171212\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.155719\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.176502\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.210483\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.053026\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.155796\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.245982\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.362414\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.155471\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.157993\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.250036\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.249121\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.106110\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.193458\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.234488\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.385387\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.102553\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.135367\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.343579\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.157321\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.238754\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.105325\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.232387\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.204197\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.321473\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.171775\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.164355\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.164841\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.139084\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.354855\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.186826\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.152580\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.279154\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.348894\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.335825\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.113747\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.218348\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.107838\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.143316\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.173935\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.161863\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.053069\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.087835\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.227163\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.123499\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.078328\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.465983\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.083474\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.300405\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.082687\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.154351\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.118101\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.096087\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.096572\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.204077\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.147631\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.276856\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.069733\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.152418\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.103929\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.131427\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.125968\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.144828\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.174443\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.211551\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.162597\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.217099\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.212400\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.163797\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.365895\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.286201\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.211087\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.221800\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.080433\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.319497\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.148627\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.179188\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.098556\n",
      "\n",
      "Test set: Average loss: 0.1802, Accuracy: 9457/10000 (94%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.117158\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.401236\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.260942\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.229537\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.302604\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.253238\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.295733\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.073378\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.267997\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.187265\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.169525\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.210875\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.040755\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.100365\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.146160\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.287770\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.092933\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.186987\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.103804\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.204644\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.275194\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.056783\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.122096\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.321040\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.213699\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.114177\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.291366\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.210870\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.104525\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.143130\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.147103\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.113338\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.460160\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.158661\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.260407\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.135219\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.173724\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.096629\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.058936\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.300155\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.162179\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.198965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.160352\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.065645\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.150738\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.113210\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.158056\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.233650\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.190896\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.132681\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.289054\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.204568\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.155830\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.185546\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.189577\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.115025\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.110519\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.439417\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.412079\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.133531\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.179257\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.366123\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.118379\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.186901\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.215999\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.128699\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.264681\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.160695\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.109761\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.068006\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.189766\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.251399\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.141483\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.264145\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.091320\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.187560\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.227514\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.131843\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.113488\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.216931\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.196305\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.082414\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.174876\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.139792\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.143882\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.177154\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.065094\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.196848\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.268526\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.265029\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.093700\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.156201\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.132221\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.094841\n",
      "\n",
      "Test set: Average loss: 0.1668, Accuracy: 9481/10000 (94%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.329396\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.366967\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.326025\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.217081\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.432484\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.095773\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.081048\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.256637\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.086583\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.256110\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.113185\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.123715\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.126210\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.077993\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.180984\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.125654\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.144934\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.170141\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.091374\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.269272\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.138061\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.217683\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.047142\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.099851\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.142170\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.044939\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.103909\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.162127\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.243693\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.150748\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.186527\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.154350\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.070852\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.213639\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.218563\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.284541\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.155155\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.116536\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.245252\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.096168\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.076837\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.263647\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.139255\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.270432\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.180631\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.316863\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.149010\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.144872\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.152089\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.078262\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.168291\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.133129\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.084499\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.116331\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.252016\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.116358\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.132484\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.256117\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.362925\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.065560\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.069505\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.142645\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.136776\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.226006\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.221494\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.246289\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.287173\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.182543\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.250813\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.101593\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.198341\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.064393\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.262531\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.154186\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.287348\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.345788\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.097558\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.075217\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.082555\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.198491\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.082040\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.230579\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.121889\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.102562\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.321706\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.296126\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.101202\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.103480\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.296542\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.137959\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.047862\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.127857\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.119209\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.178815\n",
      "\n",
      "Test set: Average loss: 0.1499, Accuracy: 9543/10000 (95%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.176509\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.132562\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.065282\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.351971\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.279892\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.205021\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.088926\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.092510\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.086655\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.105297\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.086729\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.087358\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.046487\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.137409\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.233211\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.205787\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.189528\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.168667\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.051970\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.165505\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.133386\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.049586\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.098647\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.136477\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.198168\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.102238\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.163318\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.209901\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.095656\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.116734\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.085153\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.172157\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.124829\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.215800\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.088297\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.111213\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.116924\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.155404\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.097875\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.072886\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.266210\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.244078\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.263225\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.202391\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.054435\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.211874\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.323801\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.097254\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.117643\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.329457\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.352670\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.089571\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.102772\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.106337\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.072019\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.095884\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.168759\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.084248\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.162296\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.183968\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.097503\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.123555\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.157303\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.095636\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.256791\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.091142\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.209223\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.130678\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.080134\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.168348\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.307317\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.066476\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.162687\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.145964\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.025201\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.251339\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.117258\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.071088\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.232937\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.145739\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.018198\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.089585\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.120212\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.225108\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.134369\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.122540\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.086861\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.128181\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.052604\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.065170\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.171450\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.248384\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.164574\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.155304\n",
      "\n",
      "Test set: Average loss: 0.1362, Accuracy: 9570/10000 (95%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.162539\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.140316\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.262016\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.074984\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.131247\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.150698\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.195088\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.158603\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.346404\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.404382\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.216645\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.142164\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.164749\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.264958\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.052621\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.205002\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.031453\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.106287\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.196091\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.159843\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.311511\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.105982\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.120063\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.135964\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.067941\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.165362\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.041730\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.077351\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.051593\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.086635\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.167117\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.163892\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.078260\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.079867\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.216190\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.190636\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.107778\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.131463\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.055049\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.089055\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.140787\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.275222\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.278755\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.252607\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.075297\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.354184\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.160648\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.164879\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.026129\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.111910\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.247736\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.224992\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.185584\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.070856\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.153298\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.043135\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.113700\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.186926\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.087119\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.117274\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.114901\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.082188\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.060699\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.188144\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.128399\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.150328\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.137606\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.053128\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.157262\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.203259\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.530481\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.149082\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.142880\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.169478\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.281599\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.160542\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.126250\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.082142\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.194467\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.119409\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.132240\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.089823\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.182763\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.109831\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.069203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.201556\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.082694\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.214892\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.221405\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.088034\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.283713\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.055034\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.095313\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.132579\n",
      "\n",
      "Test set: Average loss: 0.1312, Accuracy: 9580/10000 (95%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.170572\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.117488\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.162322\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.133190\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.086222\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.216163\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.100196\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.193285\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.115319\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.101174\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.124139\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.107283\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.255898\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.165122\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.050073\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.036235\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.240878\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.088962\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.084420\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.076391\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.268654\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.093138\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.322841\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.183283\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.230453\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.056755\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.062661\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.170577\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.029622\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.021103\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.066337\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.092262\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.076472\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.277523\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.162920\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.159807\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.150751\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.258611\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.189959\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.212985\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.124261\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.048598\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.152838\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.065539\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.041697\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.132606\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.077244\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.109643\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.174866\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.174650\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.223995\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.075578\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.079078\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.126383\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.107338\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.273141\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.113896\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.061319\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.114275\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.250401\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.234402\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.217069\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.179397\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.203746\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.174118\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.251957\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.084003\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.289603\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.118618\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.111724\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.249901\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.082848\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.097656\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.156702\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.103289\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.240138\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.097771\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.066551\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.036398\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.209566\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.107662\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.141438\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.133668\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.075324\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.144535\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.154840\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.059224\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.196860\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.247110\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.168821\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.072246\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.148895\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.068442\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.257442\n",
      "\n",
      "Test set: Average loss: 0.1295, Accuracy: 9587/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############# MNIST 데이터 로딩 및 신경망 정의##########################\n",
    "\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "# MNIST 데이터 셋을 가져와 train, test 구분\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                                train=True,\n",
    "                                transform=transforms.ToTensor(),\n",
    "                                download=True)\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                                train=False,\n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "# 각 데이터 셋에서 용도에 맞게 데이터를 랜덤하게 섞어서 Batch size 수에 맞게 가져온다\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)\n",
    "\n",
    "# 신경망 구조 정의\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    # 초기화\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1_out_np = np.zeros((1, 3, 24, 24))\n",
    "        self.mp1_out_np = np.zeros((1, 3, 12, 12))\n",
    "        self.conv2_out_np = np.zeros((1, 3, 8, 8))\n",
    "        self.mp2_out_np = np.zeros((1, 3, 4, 4))\n",
    "        self.fc_in_np = np.zeros((1, 48))\n",
    "        self.fc_out_np = np.zeros((1, 10))\n",
    "        \n",
    "        # 커널 크기가 5, 입력 채널이 1, 출력 채널이 3인 CNN 층 구성\n",
    "        # 출력 채널은 커널의 개수를 의미\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size=5) # Num of weight = \n",
    "        \n",
    "        # 커널 크기가 5, 입력 채널이 3, 출력 채널이 3인 CNN 층 구성\n",
    "        self.conv2 = nn.Conv2d(3, 3, kernel_size=5)\n",
    "        \n",
    "        # Max Pooling Layer, 파라미터는 kernel size 의미, 4개의 값 중 최댓값을 출력한다.\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc_1 = nn.Linear(48, 10) # Num of Weight = 480\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 텐서를 일자로 펴기 위해 x의 한 원소의 크기 계산\n",
    "        in_size = x.size(0)\n",
    "        \n",
    "        # CNN 층 출력이 최대 풀링 층을 지나 활성함수 ReLU를 지난다.\n",
    "        x = self.conv1(x)\n",
    "        self.conv1_out_np = x.detach().numpy()\n",
    "        \n",
    "        x = F.relu(self.mp(x))\n",
    "        self.mp1_out_np = x.detach().numpy()\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        self.conv2_out_np = x.detach().numpy()\n",
    "        \n",
    "        x = F.relu(self.mp(x))\n",
    "        self.mp2_out_np = x.detach().numpy()\n",
    "        \n",
    "        # 텐서를 Fully Connected Layer에 넣기 위해 일자로 편다\n",
    "        x = x.view(in_size, -1) # flatten the tensor\n",
    "        self.fc_in_np = x.detach().numpy()\n",
    "        \n",
    "        # 일자로 편 텐서로 Fully Connected Layer 계산\n",
    "        x = self.fc_1(x)\n",
    "        self.fc_out_np = x.detach().numpy()\n",
    "        \n",
    "        # 출력층의 활성함수는 Softmax 사용\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "# Instantiation    \n",
    "model = Net()\n",
    "\n",
    "# 최적화 방법으로 Momentum(= 0.5) 방식을 추가한 Stochastical Gradient Descent를 사용한다.\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "################### 신경망 학습 및 실행 ##################################\n",
    "\n",
    "def train(epoch):\n",
    "    # 해당 모델을 training 상태로 설정하는 함수\n",
    "    model.train()\n",
    "    \n",
    "    # train_loader로 batch_idx, data, target 각각 반복마다 \n",
    "    # train_loader의 인덱스를 하나씩 늘려가며 data를 넣는다.\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        #print(batch_idx)\n",
    "        \n",
    "        # Pytorch에서 다룰 수 있게 Variable로 변경\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        #print(np.shape(data))\n",
    "        \n",
    "        # 다음 최적화를 위해 전에 저장되어있던 값을 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # model을 feed-forward 되어 나온 출력값\n",
    "        output = model(data)\n",
    "        \n",
    "        # Negative Log Likelihood 로 Loss를 계산 \n",
    "        loss = F.nll_loss(output, target)\n",
    "        \n",
    "        # Gradient 값을 계산한다.\n",
    "        loss.backward()\n",
    "        \n",
    "        # back propagation을 통한 가중치값 최적화를 한번 진행\n",
    "        optimizer.step()\n",
    "        \n",
    "        # batch_idx가 10의 배수일 때 => 10번마다\n",
    "        if batch_idx % 10 == 0:\n",
    "            #각 값을 모니터 출력\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                # Percentage로 나타내기 위해 100을 곱함\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "                # data[0] is deprecated\n",
    "\n",
    "# 테스트에 사용할 함수 정의\n",
    "def test():\n",
    "    # 해당 모델을 evaluation 상태로 설정하는 함수\n",
    "    model.eval()\n",
    "    # loss 값 stacking 위해 초기화\n",
    "    test_loss = 0\n",
    "    # 맞춘 수를 stacking 위해 초기화\n",
    "    correct = 0\n",
    "    \n",
    "    \n",
    "    for data, target in test_loader:\n",
    "        \n",
    "        # 각 배열을 PyTorch로 다룰 수 있는 Variable로 만들어준다\n",
    "        # volatile was removed and now has no effect\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        \n",
    "        # model을 feed-forward 되어 나온 출력값\n",
    "        output = model(data)\n",
    "        \n",
    "        # sum up batch loss\n",
    "        #loss 값 stacking\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        \n",
    "        # get the index of the max log-probability\n",
    "        # max() : 1과 출력 data 중 최대값 출력\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        \n",
    "        # view_as(pred) == view(pred.size())\n",
    "        # => 같은 데이터를 갖는 텐서 배열을 1*n 의 새로운 배열의 텐서로 반환한다.\n",
    "        # cpu() : CUDA 처리가 불가능하여 CPU로 처리하게 하는 함수\n",
    "        # sum() : 텐서의 모든 원소 합 반환\n",
    "        # eq() : 입력받은 두 배열을 비교하여 대응되는 원소값이 갇으면 1,\n",
    "        # 다르면 0을 갖는 같은 크기의 배열을 반환한다\n",
    "        # pred와 target 비교하여 둘이 같으면 1을 갖는 배열의 합, 즉 pred 와 target이 맞는 수를 센다 \n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    # 데이터 셋의 크기로 Loss를 나눠준다    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    # 각 결과값을 모니터 출력\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:90: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "################# 샘플 통과 후 결과값 추출 #################\n",
    "from PIL import Image\n",
    "\n",
    "# 샘플을 이용한 테스트\n",
    "def sample_test():\n",
    "    # 해당 모델을 evaluation 상태로 설정하는 함수\n",
    "    model.eval()\n",
    "    # loss 값 stacking 위해 초기화\n",
    "    test_loss = 0\n",
    "    # 맞춘 수를 stacking 위해 초기화\n",
    "    correct = 0\n",
    "    \n",
    "    ############## 테스트 샘플 (0 한장) 불러오기 ####################\n",
    "    \n",
    "    # target 생성 (label : 0)\n",
    "    target = Variable(torch.tensor([0]))\n",
    "    \n",
    "    # data 생성 (0_0.bmp)\n",
    "    img = Image.open(\"C:\\\\Users\\\\kilin\\\\Desktop\\\\MNIST_CNN\\\\TEST_MNIST_SAMPLE\\\\0_0.bmp\", \"r\")\n",
    "    np_img = np.array(img)\n",
    "    np_img_re = np.reshape(np_img, (1,1,28,28))\n",
    "    \n",
    "    # 0 - 255 => 0 - 1 로 정규화, np.array => tensor 변환\n",
    "    data = Variable(torch.tensor((np_img_re / 255), dtype = torch.float32))\n",
    "    \n",
    "    # model을 feed-forward 되어 나온 출력값\n",
    "    output = model(data)\n",
    "    #print(np.shape(output))\n",
    "    \n",
    "    # sum up batch loss\n",
    "    # loss 값 stacking\n",
    "    test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "    \n",
    "    # get the index of the max log-probability\n",
    "    # max() : 1과 출력 data 중 최대값 출력\n",
    "    pred = output.data.max(1, keepdim=True)[1]\n",
    "    \n",
    "    # view_as(pred) == view(pred.size())\n",
    "    # => 같은 데이터를 갖는 텐서 배열을 1*n 의 새로운 배열의 텐서로 반환한다.\n",
    "    # cpu() : CUDA 처리가 불가능하여 CPU로 처리하게 하는 함수\n",
    "    # sum() : 텐서의 모든 원소 합 반환\n",
    "    # eq() : 입력받은 두 배열을 비교하여 대응되는 원소값이 갇으면 1,\n",
    "    # 다르면 0을 갖는 같은 크기의 배열을 반환한다\n",
    "    # pred와 target 비교하여 둘이 같으면 1을 갖는 배열의 합, 즉 pred 와 target이 맞는 수를 센다 \n",
    "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    print(correct)\n",
    "    \n",
    "    # 데이터 셋의 크기로 Loss를 나눠준다    \n",
    "    #test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    # 각 결과값을 모니터 출력\n",
    "    #print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        #test_loss, correct, len(test_loader.dataset),\n",
    "        #100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "sample_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signed\n",
      "tensor([[-25,  16,  -1,  29,  19],\n",
      "        [-11,  -3,   1,  43,  31],\n",
      "        [ 13,   5,  62,  56,  23],\n",
      "        [ -3,  34, 105,  67,   4],\n",
      "        [ -1,  43,  52,  74,  45]], dtype=torch.int32)\n",
      "tensor([[ -8, -27, -41,  -7, -31],\n",
      "        [ -7, -30, -80, -63, -61],\n",
      "        [ 56, -12, -25, -25,   0],\n",
      "        [ 68,  61,  96,  62,  19],\n",
      "        [ 28,  53,  56,  58,  68]], dtype=torch.int32)\n",
      "tensor([[ 55,  36,  33, -13, -30],\n",
      "        [ 40,   7,  14,   5, -18],\n",
      "        [ 37,  25,  -9,   7, -19],\n",
      "        [  0,  18,   1,  33,  61],\n",
      "        [ 12,  40,  53,  34,  62]], dtype=torch.int32)\n",
      "tensor([  1, -20,   0], dtype=torch.int32)\n",
      "Unsigned\n",
      "tensor([[231,  16, 255,  29,  19],\n",
      "        [245, 253,   1,  43,  31],\n",
      "        [ 13,   5,  62,  56,  23],\n",
      "        [253,  34, 105,  67,   4],\n",
      "        [255,  43,  52,  74,  45]], dtype=torch.int32)\n",
      "tensor([[248, 229, 215, 249, 225],\n",
      "        [249, 226, 176, 193, 195],\n",
      "        [ 56, 244, 231, 231,   0],\n",
      "        [ 68,  61,  96,  62,  19],\n",
      "        [ 28,  53,  56,  58,  68]], dtype=torch.int32)\n",
      "tensor([[ 55,  36,  33, 243, 226],\n",
      "        [ 40,   7,  14,   5, 238],\n",
      "        [ 37,  25, 247,   7, 237],\n",
      "        [  0,  18,   1,  33,  61],\n",
      "        [ 12,  40,  53,  34,  62]], dtype=torch.int32)\n",
      "tensor([  1, 236,   0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "############## Conv1 가중치 값 HEX 추출 ############\n",
    "\n",
    "# float => int\n",
    "int_conv1_weight_1 =  torch.tensor((model.conv1.weight.data[0][0] * 128), dtype = torch.int32)\n",
    "int_conv1_weight_2 =  torch.tensor((model.conv1.weight.data[1][0] * 128), dtype = torch.int32)\n",
    "int_conv1_weight_3 =  torch.tensor((model.conv1.weight.data[2][0] * 128), dtype = torch.int32)\n",
    "int_conv1_bias = torch.tensor((model.conv1.bias.data * 128), dtype = torch.int32)\n",
    "\n",
    "print(\"Signed\")\n",
    "print(int_conv1_weight_1)\n",
    "print(int_conv1_weight_2)\n",
    "print(int_conv1_weight_3)\n",
    "print(int_conv1_bias)\n",
    "\n",
    "# signed int => unsigned int\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if int_conv1_weight_1[i][j] < 0:\n",
    "            int_conv1_weight_1[i][j] += 256\n",
    "        if int_conv1_weight_2[i][j] < 0:\n",
    "            int_conv1_weight_2[i][j] += 256\n",
    "        if int_conv1_weight_3[i][j] < 0:\n",
    "            int_conv1_weight_3[i][j] += 256\n",
    "\n",
    "for i in range(3):\n",
    "    if int_conv1_bias[i] < 0:\n",
    "            int_conv1_bias[i] += 256\n",
    "\n",
    "print (\"Unsigned\")\n",
    "print(int_conv1_weight_1)\n",
    "print(int_conv1_weight_2)\n",
    "print(int_conv1_weight_3)\n",
    "print(int_conv1_bias)\n",
    "\n",
    "np.savetxt('conv1_weight_1.txt', int_conv1_weight_1, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv1_weight_2.txt', int_conv1_weight_2, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv1_weight_3.txt', int_conv1_weight_3, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv1_bias.txt', int_conv1_bias, fmt='%1.2x',delimiter = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 5, 5])\n",
      "Signed\n",
      "tensor([[ 20,  60,   2,  -5, -24],\n",
      "        [ 28,  48,  -5,   2,   6],\n",
      "        [ 20,  26,  -1,  25,   9],\n",
      "        [ -2,   1, -29, -24, -20],\n",
      "        [-37, -47, -57, -24, -36]], dtype=torch.int32)\n",
      "tensor([[-35, -31,   4,  -8,  -7],\n",
      "        [-47, -23,  11,  19,  34],\n",
      "        [-63,  -6,  72,  43,  36],\n",
      "        [-30,  30,  49,  14,   2],\n",
      "        [-37,  20,  30,   4,  -3]], dtype=torch.int32)\n",
      "tensor([[ 21,  21,  10,  -8, -10],\n",
      "        [  5,  19,  47,  34,  38],\n",
      "        [ 17,  14,  63,  40,  25],\n",
      "        [  5,  25,  15,   9, -10],\n",
      "        [-17, -21,   0,   4, -27]], dtype=torch.int32) \n",
      "\n",
      "tensor([[  4,  11,   4, -10,   4],\n",
      "        [-15, -20,  -5,   0,   6],\n",
      "        [  2, -31, -51, -29, -16],\n",
      "        [ 12,  24,   2, -15, -41],\n",
      "        [-20,  16,  10,  18,   8]], dtype=torch.int32)\n",
      "tensor([[-27, -13,  -6,  -9, -18],\n",
      "        [ -3,   0,   0,   5,  15],\n",
      "        [ 16,   6, -17,  -5,  -9],\n",
      "        [ 23,  56,  65,  27,  13],\n",
      "        [-14,  18,  61,  60,  87]], dtype=torch.int32)\n",
      "tensor([[-15,  -5,  12,   5,   4],\n",
      "        [-10, -17,   7,   5,   5],\n",
      "        [-12, -20, -21, -19, -18],\n",
      "        [ 33,  28,  16, -18,   2],\n",
      "        [  4,  18,  17,  28,  26]], dtype=torch.int32) \n",
      "\n",
      "tensor([[  3,  -6,  -6, -16,  18],\n",
      "        [-30, -51, -18,  40,  65],\n",
      "        [-24,  40,  59,  52,  22],\n",
      "        [ 16,  18,  -8, -27,   0],\n",
      "        [  3, -25, -65, -43, -35]], dtype=torch.int32)\n",
      "tensor([[ -5,   0, -22, -29,  -7],\n",
      "        [  2,   1,  23,  24,  31],\n",
      "        [ 12,  36,  54,  13,  10],\n",
      "        [  8,  -1,  17,  -4,  13],\n",
      "        [-13,  13,  33,  25,  12]], dtype=torch.int32)\n",
      "tensor([[  1, -14, -11,   5,  20],\n",
      "        [-28,   9,  28,  35,  15],\n",
      "        [-10,  18,  -8, -27, -13],\n",
      "        [-15, -15,  -8,  27,  13],\n",
      "        [-21, -17,   6,   3, -10]], dtype=torch.int32) \n",
      "\n",
      "tensor([-4, -5,  2], dtype=torch.int32)\n",
      "Unsigned\n",
      "tensor([[ 20,  60,   2, 251, 232],\n",
      "        [ 28,  48, 251,   2,   6],\n",
      "        [ 20,  26, 255,  25,   9],\n",
      "        [254,   1, 227, 232, 236],\n",
      "        [219, 209, 199, 232, 220]], dtype=torch.int32)\n",
      "tensor([[221, 225,   4, 248, 249],\n",
      "        [209, 233,  11,  19,  34],\n",
      "        [193, 250,  72,  43,  36],\n",
      "        [226,  30,  49,  14,   2],\n",
      "        [219,  20,  30,   4, 253]], dtype=torch.int32)\n",
      "tensor([[ 21,  21,  10, 248, 246],\n",
      "        [  5,  19,  47,  34,  38],\n",
      "        [ 17,  14,  63,  40,  25],\n",
      "        [  5,  25,  15,   9, 246],\n",
      "        [239, 235,   0,   4, 229]], dtype=torch.int32) \n",
      "\n",
      "tensor([[  4,  11,   4, 246,   4],\n",
      "        [241, 236, 251,   0,   6],\n",
      "        [  2, 225, 205, 227, 240],\n",
      "        [ 12,  24,   2, 241, 215],\n",
      "        [236,  16,  10,  18,   8]], dtype=torch.int32)\n",
      "tensor([[229, 243, 250, 247, 238],\n",
      "        [253,   0,   0,   5,  15],\n",
      "        [ 16,   6, 239, 251, 247],\n",
      "        [ 23,  56,  65,  27,  13],\n",
      "        [242,  18,  61,  60,  87]], dtype=torch.int32)\n",
      "tensor([[241, 251,  12,   5,   4],\n",
      "        [246, 239,   7,   5,   5],\n",
      "        [244, 236, 235, 237, 238],\n",
      "        [ 33,  28,  16, 238,   2],\n",
      "        [  4,  18,  17,  28,  26]], dtype=torch.int32) \n",
      "\n",
      "tensor([[  3, 250, 250, 240,  18],\n",
      "        [226, 205, 238,  40,  65],\n",
      "        [232,  40,  59,  52,  22],\n",
      "        [ 16,  18, 248, 229,   0],\n",
      "        [  3, 231, 191, 213, 221]], dtype=torch.int32)\n",
      "tensor([[251,   0, 234, 227, 249],\n",
      "        [  2,   1,  23,  24,  31],\n",
      "        [ 12,  36,  54,  13,  10],\n",
      "        [  8, 255,  17, 252,  13],\n",
      "        [243,  13,  33,  25,  12]], dtype=torch.int32)\n",
      "tensor([[  1, 242, 245,   5,  20],\n",
      "        [228,   9,  28,  35,  15],\n",
      "        [246,  18, 248, 229, 243],\n",
      "        [241, 241, 248,  27,  13],\n",
      "        [235, 239,   6,   3, 246]], dtype=torch.int32) \n",
      "\n",
      "tensor([252, 251,   2], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "############## Conv2 가중치 값 HEX 추출 ############\n",
    "\n",
    "print(np.shape(model.conv2.weight))\n",
    "\n",
    "# float => int\n",
    "int_conv2_weight_11 =  torch.tensor((model.conv2.weight.data[0][0]* 128), dtype = torch.int32)\n",
    "int_conv2_weight_12 =  torch.tensor((model.conv2.weight.data[0][1]* 128), dtype = torch.int32)\n",
    "int_conv2_weight_13 =  torch.tensor((model.conv2.weight.data[0][2]* 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_weight_21 =  torch.tensor((model.conv2.weight.data[1][0] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_22 =  torch.tensor((model.conv2.weight.data[1][1] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_23 =  torch.tensor((model.conv2.weight.data[1][2] * 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_weight_31 =  torch.tensor((model.conv2.weight.data[2][0] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_32 =  torch.tensor((model.conv2.weight.data[2][1] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_33 =  torch.tensor((model.conv2.weight.data[2][2] * 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_bias = torch.tensor((model.conv2.bias.data * 128), dtype = torch.int32)\n",
    "\n",
    "print (\"Signed\")\n",
    "print(int_conv2_weight_11)\n",
    "print(int_conv2_weight_12)\n",
    "print(int_conv2_weight_13, '\\n')\n",
    "\n",
    "print(int_conv2_weight_21)\n",
    "print(int_conv2_weight_22)\n",
    "print(int_conv2_weight_23, '\\n')\n",
    "\n",
    "print(int_conv2_weight_31)\n",
    "print(int_conv2_weight_32)\n",
    "print(int_conv2_weight_33, '\\n')\n",
    "\n",
    "print(int_conv2_bias)\n",
    "\n",
    "\n",
    "# signed int => unsigned int\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if int_conv2_weight_11[i][j] < 0:\n",
    "            int_conv2_weight_11[i][j] += 256\n",
    "        if int_conv2_weight_12[i][j] < 0:\n",
    "            int_conv2_weight_12[i][j] += 256\n",
    "        if int_conv2_weight_13[i][j] < 0:\n",
    "            int_conv2_weight_13[i][j] += 256\n",
    "            \n",
    "        if int_conv2_weight_21[i][j] < 0:\n",
    "            int_conv2_weight_21[i][j] += 256\n",
    "        if int_conv2_weight_22[i][j] < 0:\n",
    "            int_conv2_weight_22[i][j] += 256\n",
    "        if int_conv2_weight_23[i][j] < 0:\n",
    "            int_conv2_weight_23[i][j] += 256\n",
    "            \n",
    "        if int_conv2_weight_31[i][j] < 0:\n",
    "            int_conv2_weight_31[i][j] += 256\n",
    "        if int_conv2_weight_32[i][j] < 0:\n",
    "            int_conv2_weight_32[i][j] += 256\n",
    "        if int_conv2_weight_33[i][j] < 0:\n",
    "            int_conv2_weight_33[i][j] += 256\n",
    "\n",
    "for i in range(3):\n",
    "    if int_conv2_bias[i] < 0:\n",
    "            int_conv2_bias[i] += 256\n",
    "\n",
    "print (\"Unsigned\")\n",
    "print(int_conv2_weight_11)\n",
    "print(int_conv2_weight_12)\n",
    "print(int_conv2_weight_13, '\\n')\n",
    "\n",
    "print(int_conv2_weight_21)\n",
    "print(int_conv2_weight_22)\n",
    "print(int_conv2_weight_23, '\\n')\n",
    "\n",
    "print(int_conv2_weight_31)\n",
    "print(int_conv2_weight_32)\n",
    "print(int_conv2_weight_33, '\\n')\n",
    "\n",
    "print(int_conv2_bias)\n",
    "\n",
    "np.savetxt('conv2_weight_11.txt', int_conv2_weight_11, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_12.txt', int_conv2_weight_12, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_13.txt', int_conv2_weight_13, fmt='%1.2x',delimiter = \" \")\n",
    "\n",
    "np.savetxt('conv2_weight_21.txt', int_conv2_weight_21, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_22.txt', int_conv2_weight_22, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_23.txt', int_conv2_weight_23, fmt='%1.2x',delimiter = \" \")\n",
    "\n",
    "np.savetxt('conv2_weight_31.txt', int_conv2_weight_31, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_32.txt', int_conv2_weight_32, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_33.txt', int_conv2_weight_33, fmt='%1.2x',delimiter = \" \")\n",
    "\n",
    "np.savetxt('conv2_bias.txt', int_conv2_bias, fmt='%1.2x',delimiter = \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 48])\n",
      "tensor([[-14,  13,  22,  10, -16, -14, -26,  19,   1,  33, -18,   4,  41,  40,\n",
      "          -1, -14,  17,  -2,  -9,   4,  -3, -10, -42, -41, -15,  -3, -19, -12,\n",
      "          14,  26,  27, -24, -12,  11,   3,  -1,  14, -36, -49,  -8,  -7, -40,\n",
      "           2,  46,   6,  -4,  19,  27],\n",
      "        [  3,  -7, -24,  -4,   5, -40,  58, -11, -18, -31,  49,  10,   0, -33,\n",
      "          52, -28, -13,  -5,   1, -57, -29, -22, -18, -29,  -2,   1, -26,  53,\n",
      "           5, -41, -19,  44, -14, -15, -25,  23,  -5,  40, -18, -28,  -7,  33,\n",
      "         -19,  18,  40, -14,   3,   8],\n",
      "        [ 30,  35,   2, -27, -44, -37, -36,  19, -39, -44, -18,  18,  19,  47,\n",
      "           1,   6,  45,  31,  -7, -24,   8,   5,   4,   4,  14, -33,   1,  60,\n",
      "          -9,   9,   8,  50,  15, -10, -24, -56,  23, -10,   5,  -3,  28,   8,\n",
      "          32,  48,   3, -17, -24,  24],\n",
      "        [ 67,  16,  -8,   4, -22, -45,  13,   8,   3,  -5,  13, -27, -27, -34,\n",
      "         -31,  -3, -34,  -3,   0, -15,   5,  22,  34,  30,  55,  10,   5, -16,\n",
      "          35,  -5,  -8, -16,   8,  -6, -20, -23,  46,  63,  73, -20,  -2,  -8,\n",
      "         -13, -33,   1,   0,  24,  -8],\n",
      "        [-58, -38, -41,   3,   7,  60,  24,  38,  34,  48, -18,  54,  -6, -30,\n",
      "           1,  15, -11,   0, -25, -56,  20,   2,   9, -13, -30,  10,  11,  -8,\n",
      "          -4,  -9, -35,   5,  -2, -66, -46,  27,  37,   5, -32,   3, -14, -13,\n",
      "          32,  12,  -8,   8, -19, -18],\n",
      "        [-28,   5,  18,  36,  21,  42,  -8, -75,  13,  16, -16, -29, -16, -10,\n",
      "         -32, -19, -19,   2,  23,  45,  20,   7,   3,  43,  56,  -2,   6,  -1,\n",
      "          15,  29,   0,   5,  -5,  13,  25,  49,  -2, -20,  -3, -22,  18, -21,\n",
      "         -28, -58,  11,  13,   0, -19],\n",
      "        [-43, -52,  -4,   2, -22,  18, -27, -68,   0,  26,   5, -39,  38,  69,\n",
      "          26,  12, -16, -10,   7,  58, -26,   0,   9,  35, -21,  15, -13, -12,\n",
      "         -39, -21,   5, -20, -33,  -3,  36,  -4, -11, -47,  17,  19, -24, -12,\n",
      "         -31,  23, -12,  -7,  15,   0],\n",
      "        [ 64,  63,   6,  14,   8, -23, -13,  36,  -4, -24, -20,  57, -30, -43,\n",
      "         -17,  -1,  28, -21,   0,  13,  -8, -16,   5,  13, -37,   4,   4, -19,\n",
      "         -11, -35, -23, -28,   2,   6,  16,  34,  11,   3,  -3,   0,  21, -16,\n",
      "           8,   0,  -1,  14, -28, -20],\n",
      "        [-31,   8,  13,  -3,  39,  41,  -5,  19, -30, -29,  24, -38,  22,  64,\n",
      "          -3,   3,  -3,  23, -15, -16, -11,   3,  17,   0,  -7, -34, -20,   9,\n",
      "           3,  16,  21,  -2,  25,  12,  -3,   2,  -1, -45,  28,  28, -11,  35,\n",
      "          -4, -15, -35, -29, -13,  -9],\n",
      "        [-49,  -4,  26, -15,   0,  61,  -5,   1,  43,  33,   6,  28, -14, -53,\n",
      "         -43,  22,  10,   2,  -4, -17,  15,  14, -25, -55, -11,  33,  -6, -32,\n",
      "           0,  14, -21,   4,  21,  53,  13, -26, -33,  -4,  -1, -17, -30,   9,\n",
      "          -8,  -9,  -7,  19, -32,  -9]], dtype=torch.int32)\n",
      "torch.Size([10])\n",
      "tensor([ 3,  7, -2, -6,  0,  0,  9,  0, -9, -8], dtype=torch.int32)\n",
      "tensor([[242,  13,  22,  10, 240, 242, 230,  19,   1,  33, 238,   4,  41,  40,\n",
      "         255, 242,  17, 254, 247,   4, 253, 246, 214, 215, 241, 253, 237, 244,\n",
      "          14,  26,  27, 232, 244,  11,   3, 255,  14, 220, 207, 248, 249, 216,\n",
      "           2,  46,   6, 252,  19,  27],\n",
      "        [  3, 249, 232, 252,   5, 216,  58, 245, 238, 225,  49,  10,   0, 223,\n",
      "          52, 228, 243, 251,   1, 199, 227, 234, 238, 227, 254,   1, 230,  53,\n",
      "           5, 215, 237,  44, 242, 241, 231,  23, 251,  40, 238, 228, 249,  33,\n",
      "         237,  18,  40, 242,   3,   8],\n",
      "        [ 30,  35,   2, 229, 212, 219, 220,  19, 217, 212, 238,  18,  19,  47,\n",
      "           1,   6,  45,  31, 249, 232,   8,   5,   4,   4,  14, 223,   1,  60,\n",
      "         247,   9,   8,  50,  15, 246, 232, 200,  23, 246,   5, 253,  28,   8,\n",
      "          32,  48,   3, 239, 232,  24],\n",
      "        [ 67,  16, 248,   4, 234, 211,  13,   8,   3, 251,  13, 229, 229, 222,\n",
      "         225, 253, 222, 253,   0, 241,   5,  22,  34,  30,  55,  10,   5, 240,\n",
      "          35, 251, 248, 240,   8, 250, 236, 233,  46,  63,  73, 236, 254, 248,\n",
      "         243, 223,   1,   0,  24, 248],\n",
      "        [198, 218, 215,   3,   7,  60,  24,  38,  34,  48, 238,  54, 250, 226,\n",
      "           1,  15, 245,   0, 231, 200,  20,   2,   9, 243, 226,  10,  11, 248,\n",
      "         252, 247, 221,   5, 254, 190, 210,  27,  37,   5, 224,   3, 242, 243,\n",
      "          32,  12, 248,   8, 237, 238],\n",
      "        [228,   5,  18,  36,  21,  42, 248, 181,  13,  16, 240, 227, 240, 246,\n",
      "         224, 237, 237,   2,  23,  45,  20,   7,   3,  43,  56, 254,   6, 255,\n",
      "          15,  29,   0,   5, 251,  13,  25,  49, 254, 236, 253, 234,  18, 235,\n",
      "         228, 198,  11,  13,   0, 237],\n",
      "        [213, 204, 252,   2, 234,  18, 229, 188,   0,  26,   5, 217,  38,  69,\n",
      "          26,  12, 240, 246,   7,  58, 230,   0,   9,  35, 235,  15, 243, 244,\n",
      "         217, 235,   5, 236, 223, 253,  36, 252, 245, 209,  17,  19, 232, 244,\n",
      "         225,  23, 244, 249,  15,   0],\n",
      "        [ 64,  63,   6,  14,   8, 233, 243,  36, 252, 232, 236,  57, 226, 213,\n",
      "         239, 255,  28, 235,   0,  13, 248, 240,   5,  13, 219,   4,   4, 237,\n",
      "         245, 221, 233, 228,   2,   6,  16,  34,  11,   3, 253,   0,  21, 240,\n",
      "           8,   0, 255,  14, 228, 236],\n",
      "        [225,   8,  13, 253,  39,  41, 251,  19, 226, 227,  24, 218,  22,  64,\n",
      "         253,   3, 253,  23, 241, 240, 245,   3,  17,   0, 249, 222, 236,   9,\n",
      "           3,  16,  21, 254,  25,  12, 253,   2, 255, 211,  28,  28, 245,  35,\n",
      "         252, 241, 221, 227, 243, 247],\n",
      "        [207, 252,  26, 241,   0,  61, 251,   1,  43,  33,   6,  28, 242, 203,\n",
      "         213,  22,  10,   2, 252, 239,  15,  14, 231, 201, 245,  33, 250, 224,\n",
      "           0,  14, 235,   4,  21,  53,  13, 230, 223, 252, 255, 239, 226,   9,\n",
      "         248, 247, 249,  19, 224, 247]], dtype=torch.int32)\n",
      "tensor([  3,   7, 254, 250,   0,   0,   9,   0, 247, 248], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "############## FC Layer가중치 값 HEX 추출 ############\n",
    "\n",
    "print(np.shape(model.fc_1.weight))\n",
    "print((model.fc_1.weight * 128).int())\n",
    "\n",
    "print(np.shape(model.fc_1.bias))\n",
    "print((model.fc_1.bias * 128).int())\n",
    "\n",
    "int_fc_weight = (model.fc_1.weight * 128).int()\n",
    "int_fc_bias = (model.fc_1.bias * 128).int()\n",
    "\n",
    "# signed int => unsigned int\n",
    "for i in range(10):\n",
    "    for j in range(48):\n",
    "        if int_fc_weight[i][j] < 0 :\n",
    "            int_fc_weight[i][j] += 256\n",
    "    if int_fc_bias[i] < 0 :\n",
    "        int_fc_bias[i] += 256\n",
    "        \n",
    "print(int_fc_weight)\n",
    "print(int_fc_bias)\n",
    "\n",
    "np.savetxt('fc_weight.txt', int_fc_weight, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('fc_bias.txt', int_fc_bias, fmt='%1.2x',delimiter = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 24, 24)\n",
      "(1, 3, 12, 12)\n",
      "(1, 3, 8, 8)\n",
      "(1, 3, 4, 4)\n",
      "(1, 48)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "######################### 신경망 통과 값 추출 ###########################\n",
    "print(np.shape(model.conv1_out_np))\n",
    "np.savetxt('out_conv1_value_1.txt', model.conv1_out_np[0][0]*128, fmt='%1.5d',delimiter = \" \")\n",
    "np.savetxt('out_conv1_value_2.txt', model.conv1_out_np[0][1]*128, fmt='%1.5d',delimiter = \" \")\n",
    "np.savetxt('out_conv1_value_3.txt', model.conv1_out_np[0][2]*128, fmt='%1.5d',delimiter = \" \")\n",
    "\n",
    "print(np.shape(model.mp1_out_np))\n",
    "np.savetxt('out_mp1_value_1.txt', model.mp1_out_np[0][0]*128, fmt='%1.5d',delimiter = \" \")\n",
    "np.savetxt('out_mp1_value_2.txt', model.mp1_out_np[0][1]*128, fmt='%1.5d',delimiter = \" \")\n",
    "np.savetxt('out_mp1_value_3.txt', model.mp1_out_np[0][2]*128, fmt='%1.5d',delimiter = \" \")\n",
    "\n",
    "print(np.shape(model.conv2_out_np))\n",
    "np.savetxt('out_conv2_value_1.txt', model.conv2_out_np[0][0]*128, fmt='%1.5d',delimiter = \" \")\n",
    "np.savetxt('out_conv2_value_2.txt', model.conv2_out_np[0][1]*128, fmt='%1.5d',delimiter = \" \")\n",
    "np.savetxt('out_conv2_value_3.txt', model.conv2_out_np[0][2]*128, fmt='%1.5d',delimiter = \" \")\n",
    "\n",
    "print(np.shape(model.mp2_out_np))\n",
    "np.savetxt('out_mp2_value_1.txt', model.mp2_out_np[0][0]*128, fmt='%1.5d',delimiter = \" \")\n",
    "np.savetxt('out_mp2_value_2.txt', model.mp2_out_np[0][1]*128, fmt='%1.5d',delimiter = \" \")\n",
    "np.savetxt('out_mp2_value_3.txt', model.mp2_out_np[0][2]*128, fmt='%1.5d',delimiter = \" \")\n",
    "\n",
    "print(np.shape(model.fc_in_np))\n",
    "np.savetxt('fc_in_value.txt', model.fc_in_np*128, fmt='%1.5d',delimiter = \" \")\n",
    "\n",
    "print(np.shape(model.fc_out_np))\n",
    "np.savetxt('fc_out_value.txt', model.fc_out_np*128, fmt='%1.5d',delimiter = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Input\n",
      "[[[0.007946 0.007946 0.007946 0.007946 0.485303 2.433931 2.86805\n",
      "   0.795214 0.007946 0.007946 0.007946 0.007946]\n",
      "  [0.007946 0.007946 0.007946 0.526232 2.294008 4.878462 4.494352\n",
      "   1.776778 0.269568 0.007946 0.007946 0.007946]\n",
      "  [0.007946 0.007946 0.262226 2.138685 4.997587 5.42438  5.123318\n",
      "   3.718413 2.462729 0.392078 0.007946 0.007946]\n",
      "  [0.007946 0.052655 1.224695 4.752855 5.336329 5.290852 4.306835\n",
      "   3.687219 3.75194  1.586619 0.145947 0.007946]\n",
      "  [0.007946 0.087386 2.709602 5.223812 4.915937 3.461112 1.492861\n",
      "   2.755593 3.816344 3.520272 0.83291  0.007946]\n",
      "  [0.007946 0.289716 3.459296 4.596095 3.143979 0.517921 0.\n",
      "   1.717676 4.691989 4.296669 1.12771  0.010092]\n",
      "  [0.007946 0.49768  4.383057 4.611019 0.754726 0.       0.737615\n",
      "   3.018578 5.127934 4.624765 0.979043 0.03125 ]\n",
      "  [0.007946 0.789257 4.539495 4.514042 1.136255 1.673958 3.273121\n",
      "   5.175742 5.172872 3.462619 0.049458 0.007946]\n",
      "  [0.007946 0.688944 4.365803 4.677119 3.816629 4.618104 5.225662\n",
      "   5.287105 4.709015 1.065053 0.       0.007946]\n",
      "  [0.007946 0.433008 2.752466 4.0646   4.776063 5.170724 5.109639\n",
      "   4.075969 1.712199 0.       0.007946 0.007946]\n",
      "  [0.007946 0.10568  1.14365  2.047847 3.161655 3.379068 2.450973\n",
      "   1.007963 0.014389 0.000032 0.007946 0.007946]\n",
      "  [0.007946 0.007946 0.150138 0.67818  0.893142 0.571242 0.242472\n",
      "   0.       0.007946 0.007946 0.007946 0.007946]]\n",
      "\n",
      " [[0.       0.       0.       0.       0.538215 2.378271 2.853341\n",
      "   1.662881 0.       0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.444538 1.578348 2.717902 3.029648\n",
      "   2.342821 0.675434 0.       0.       0.      ]\n",
      "  [0.       0.       0.21716  1.646855 2.256117 2.185431 2.452804\n",
      "   3.95417  3.239146 1.142051 0.       0.      ]\n",
      "  [0.       0.       1.086046 1.686856 1.76499  1.659173 0.962013\n",
      "   2.707065 3.182972 2.623747 0.466317 0.      ]\n",
      "  [0.       0.       1.169602 1.38478  0.846087 0.       0.\n",
      "   0.       1.818687 3.062528 1.990572 0.      ]\n",
      "  [0.       0.051127 0.185618 0.       0.       0.       0.\n",
      "   0.       0.       2.261208 1.982919 0.261931]\n",
      "  [0.       0.315434 0.745832 1.015067 0.98158  0.       0.698397\n",
      "   1.575585 1.639442 1.094915 1.11702  0.203292]\n",
      "  [0.       0.121193 0.437424 1.455959 2.088096 1.812653 2.7416\n",
      "   2.504634 1.903954 0.324371 0.061894 0.      ]\n",
      "  [0.       0.       0.       2.282635 4.315901 3.980761 3.79242\n",
      "   2.257398 0.858997 0.042341 0.       0.      ]\n",
      "  [0.       0.       0.       0.97174  2.813658 2.586293 1.749484\n",
      "   0.051512 0.       0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.       0.       0.\n",
      "   0.       0.       0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.       0.       0.\n",
      "   0.       0.       0.       0.       0.      ]]\n",
      "\n",
      " [[0.       0.       0.       0.       0.81038  1.861323 1.575386\n",
      "   0.697751 0.036189 0.       0.       0.      ]\n",
      "  [0.       0.       0.       0.518092 1.647095 2.269462 2.367355\n",
      "   1.9004   0.460488 0.01924  0.       0.      ]\n",
      "  [0.       0.       0.346298 1.685723 2.032854 2.722809 4.213078\n",
      "   3.997921 1.865784 0.507415 0.       0.      ]\n",
      "  [0.       0.057294 1.14958  1.776105 2.496933 2.777928 3.268051\n",
      "   3.340427 3.07919  1.95535  0.237105 0.      ]\n",
      "  [0.       0.126767 1.295521 1.781435 2.166569 2.079577 1.533658\n",
      "   2.501032 3.411085 3.033581 1.103864 0.01454 ]\n",
      "  [0.       0.261837 1.233116 1.511067 2.112595 1.664802 1.100237\n",
      "   0.608476 2.702893 3.288435 2.026533 0.104591]\n",
      "  [0.       0.682096 1.356024 1.950965 2.372929 0.917389 0.771834\n",
      "   1.630618 2.153573 2.786132 2.152932 0.300962]\n",
      "  [0.       0.577712 1.502262 2.774825 2.751772 1.827046 2.145486\n",
      "   2.081219 1.875709 2.181576 1.835444 0.354456]\n",
      "  [0.       0.167164 1.407399 4.043074 3.877592 2.450373 2.48252\n",
      "   2.282411 2.048493 2.141749 1.214773 0.04692 ]\n",
      "  [0.       0.       0.802341 3.419141 3.566501 2.886083 2.536558\n",
      "   1.801444 1.837494 1.673055 0.303324 0.      ]\n",
      "  [0.       0.       0.       0.810595 1.841369 1.761809 1.65858\n",
      "   1.613082 1.372954 0.635438 0.       0.      ]\n",
      "  [0.       0.       0.       0.       0.480237 0.946353 1.190195\n",
      "   0.935798 0.216077 0.       0.       0.      ]]]\n",
      "Original Weight\n",
      "[[[[ 0.161964  0.470768  0.022214 -0.041867 -0.187509]\n",
      "   [ 0.220254  0.379335 -0.040522  0.015905  0.047669]\n",
      "   [ 0.163605  0.20572  -0.014322  0.200505  0.072897]\n",
      "   [-0.01708   0.00905  -0.227909 -0.190961 -0.161697]\n",
      "   [-0.290894 -0.372391 -0.452782 -0.188183 -0.282856]]\n",
      "\n",
      "  [[-0.275559 -0.245132  0.038548 -0.069348 -0.061491]\n",
      "   [-0.370164 -0.181411  0.086822  0.151522  0.273283]\n",
      "   [-0.4948   -0.05157   0.56916   0.342056  0.285216]\n",
      "   [-0.239458  0.235761  0.386367  0.110653  0.021934]\n",
      "   [-0.290839  0.162121  0.241901  0.033108 -0.02575 ]]\n",
      "\n",
      "  [[ 0.169912  0.171285  0.081674 -0.070242 -0.084373]\n",
      "   [ 0.041443  0.154035  0.370231  0.269076  0.297693]\n",
      "   [ 0.134794  0.114522  0.494094  0.312744  0.199205]\n",
      "   [ 0.043054  0.195768  0.122822  0.077571 -0.082494]\n",
      "   [-0.137635 -0.168915 -0.005863  0.035253 -0.216915]]]\n",
      "\n",
      "\n",
      " [[[ 0.038389  0.093051  0.031387 -0.080645  0.034293]\n",
      "   [-0.121765 -0.161279 -0.044499  0.001007  0.048239]\n",
      "   [ 0.023073 -0.248824 -0.404317 -0.23429  -0.127397]\n",
      "   [ 0.095328  0.188431  0.018968 -0.121287 -0.323512]\n",
      "   [-0.156832  0.131924  0.084501  0.145641  0.063174]]\n",
      "\n",
      "  [[-0.213614 -0.103937 -0.053556 -0.072284 -0.146506]\n",
      "   [-0.024116 -0.006692  0.007137  0.039246  0.123654]\n",
      "   [ 0.128871  0.05403  -0.133482 -0.041847 -0.071206]\n",
      "   [ 0.186025  0.44149   0.509709  0.218442  0.102627]\n",
      "   [-0.110181  0.143668  0.480063  0.469211  0.683035]]\n",
      "\n",
      "  [[-0.117886 -0.040303  0.100269  0.046319  0.035784]\n",
      "   [-0.082071 -0.138176  0.062013  0.039445  0.042548]\n",
      "   [-0.094257 -0.163579 -0.168215 -0.154014 -0.146424]\n",
      "   [ 0.261407  0.225657  0.130812 -0.141398  0.019674]\n",
      "   [ 0.034601  0.143911  0.138668  0.220752  0.206034]]]\n",
      "\n",
      "\n",
      " [[[ 0.024929 -0.048294 -0.049037 -0.132201  0.144641]\n",
      "   [-0.240342 -0.405528 -0.145372  0.312874  0.5154  ]\n",
      "   [-0.195168  0.313004  0.462526  0.408275  0.179341]\n",
      "   [ 0.12661   0.146111 -0.069822 -0.21156  -0.003937]\n",
      "   [ 0.028599 -0.196241 -0.509871 -0.343336 -0.27555 ]]\n",
      "\n",
      "  [[-0.04029   0.006699 -0.172641 -0.232321 -0.05902 ]\n",
      "   [ 0.018358  0.014814  0.181439  0.189279  0.243804]\n",
      "   [ 0.098406  0.281937  0.428564  0.109188  0.079515]\n",
      "   [ 0.067331 -0.010843  0.135913 -0.031675  0.108825]\n",
      "   [-0.105004  0.102412  0.260966  0.20195   0.095391]]\n",
      "\n",
      "  [[ 0.014236 -0.114976 -0.090367  0.046081  0.157155]\n",
      "   [-0.221664  0.072431  0.225806  0.280755  0.122835]\n",
      "   [-0.081522  0.146169 -0.069149 -0.213156 -0.10359 ]\n",
      "   [-0.124957 -0.123465 -0.063065  0.217865  0.103368]\n",
      "   [-0.16834  -0.134791  0.054418  0.029893 -0.085846]]]]\n",
      "Original Bias\n",
      "[-0.033113 -0.04593   0.021453]\n",
      "\n",
      "Bias : \n",
      "[-0.033113 -0.04593   0.021453]\n",
      "\n",
      "Calc Value :\n",
      "\n",
      "[[[-0.828656  0.704892  1.951454  4.398039  7.253536  7.254331  2.941463\n",
      "   -1.957194]\n",
      "  [ 0.741293  1.855783  4.447139  9.646738 12.662814 11.908774  6.842013\n",
      "    1.550577]\n",
      "  [ 0.119945  1.614863  4.51439   7.893131 10.060529 10.734887  8.299783\n",
      "    4.946329]\n",
      "  [-1.641578  0.571876  4.172332  4.380099  3.920082  4.610085  6.142312\n",
      "    6.784406]\n",
      "  [-2.08283   1.332775  4.652186  1.525881 -0.761069  0.436466  3.622391\n",
      "    7.281419]\n",
      "  [ 0.116471  5.234762  9.012966  4.75481   0.476949  0.31337   3.093289\n",
      "    7.89415 ]\n",
      "  [ 4.854157 12.622501 16.285838  9.297841  4.304612  3.844902  5.803881\n",
      "    8.757852]\n",
      "  [ 6.964129 14.440686 16.891522 11.215444  7.455585  7.052282  8.527937\n",
      "    8.914793]]\n",
      "\n",
      " [[ 1.146417  0.586187 -1.169071 -2.942539 -1.610942  2.453888  7.262852\n",
      "    8.690933]\n",
      "  [-1.834294 -2.295619 -3.389921 -5.245375 -6.100525 -3.842924  0.442188\n",
      "    4.9631  ]\n",
      "  [-1.033363 -2.016812 -2.564297 -2.824316 -2.422156 -2.91347  -2.215977\n",
      "   -0.722874]\n",
      "  [ 1.722128  2.920537  4.078177  3.532974  2.955835  0.999714 -0.060664\n",
      "   -1.781264]\n",
      "  [ 3.112078  7.19618  10.234671  9.264523  6.238125  3.213756  1.055065\n",
      "   -1.117758]\n",
      "  [-0.265266  3.330173  7.658986  7.69007   4.490305  1.306114 -0.581346\n",
      "   -0.885054]\n",
      "  [-5.848293 -5.425791 -3.476365 -1.696841 -1.445248 -2.283451 -3.213878\n",
      "   -2.133084]\n",
      "  [-4.599831 -6.495796 -7.997268 -7.80482  -6.341068 -4.92117  -3.596053\n",
      "   -1.968006]]\n",
      "\n",
      " [[-0.118788  4.133819  7.610723  5.862511  1.706493  0.098597 -0.327243\n",
      "   -0.503401]\n",
      "  [ 3.940404  8.071779  9.654884  7.656303  4.034796  0.173556 -1.783448\n",
      "   -1.273868]\n",
      "  [ 6.16888   8.220167  7.196319  2.93306  -0.960484 -2.940139 -2.509633\n",
      "   -0.97339 ]\n",
      "  [ 4.504368  3.818927 -0.449073 -3.974899 -3.636158 -1.587308 -0.581302\n",
      "   -0.031386]\n",
      "  [ 2.713547  0.207099 -4.013496 -5.390374 -1.430536  3.429735  3.168507\n",
      "    1.192962]\n",
      "  [ 3.179719 -0.775307 -3.076608 -0.816266  6.059452  9.053096  5.747655\n",
      "   -0.41945 ]\n",
      "  [ 4.607215  3.535342  5.358871  8.928098 11.408763  8.852822  2.042906\n",
      "   -3.304408]\n",
      "  [ 7.449527  8.639919 10.14406  10.064133  7.692013  1.909238 -3.412887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -4.26109 ]]]\n",
      "\n",
      "Real Value :\n",
      "\n",
      "[[[-0.828657  0.704893  1.951454  4.398038  7.253537  7.254332  2.941462\n",
      "   -1.957195]\n",
      "  [ 0.741294  1.855782  4.447139  9.646738 12.662814 11.908775  6.842013\n",
      "    1.550577]\n",
      "  [ 0.119945  1.614863  4.51439   7.893129 10.060531 10.734888  8.299784\n",
      "    4.94633 ]\n",
      "  [-1.64158   0.571877  4.172332  4.380099  3.920083  4.610086  6.142311\n",
      "    6.784405]\n",
      "  [-2.08283   1.332775  4.652186  1.525881 -0.761069  0.436467  3.622389\n",
      "    7.281419]\n",
      "  [ 0.116472  5.234763  9.012963  4.75481   0.47695   0.313369  3.093289\n",
      "    7.89415 ]\n",
      "  [ 4.854158 12.622501 16.285841  9.297841  4.304612  3.844903  5.803882\n",
      "    8.757852]\n",
      "  [ 6.964129 14.440686 16.891525 11.215445  7.455584  7.052281  8.527936\n",
      "    8.914791]]\n",
      "\n",
      " [[ 1.146417  0.586187 -1.169071 -2.942539 -1.610942  2.453888  7.262853\n",
      "    8.690933]\n",
      "  [-1.834294 -2.29562  -3.389921 -5.245375 -6.100524 -3.842926  0.442187\n",
      "    4.9631  ]\n",
      "  [-1.033363 -2.016812 -2.564297 -2.824316 -2.422157 -2.913472 -2.215978\n",
      "   -0.722873]\n",
      "  [ 1.722128  2.920537  4.078176  3.532973  2.955833  0.999714 -0.060663\n",
      "   -1.781265]\n",
      "  [ 3.112078  7.196181 10.23467   9.264524  6.238125  3.213756  1.055065\n",
      "   -1.117758]\n",
      "  [-0.265266  3.330174  7.658987  7.69007   4.490304  1.306114 -0.581345\n",
      "   -0.885054]\n",
      "  [-5.848294 -5.425792 -3.476366 -1.69684  -1.445247 -2.283452 -3.213878\n",
      "   -2.133084]\n",
      "  [-4.599832 -6.495796 -7.997268 -7.80482  -6.341067 -4.92117  -3.596054\n",
      "   -1.968006]]\n",
      "\n",
      " [[-0.118787  4.13382   7.610721  5.86251   1.706492  0.098597 -0.327243\n",
      "   -0.503401]\n",
      "  [ 3.940406  8.071779  9.654885  7.656302  4.034795  0.173555 -1.783448\n",
      "   -1.273868]\n",
      "  [ 6.16888   8.220166  7.196319  2.93306  -0.960485 -2.940138 -2.509634\n",
      "   -0.973389]\n",
      "  [ 4.504367  3.818926 -0.449073 -3.974899 -3.636158 -1.587308 -0.581302\n",
      "   -0.031386]\n",
      "  [ 2.713548  0.2071   -4.013496 -5.390374 -1.430535  3.429734  3.168506\n",
      "    1.192963]\n",
      "  [ 3.179719 -0.775308 -3.076608 -0.816266  6.059454  9.053101  5.747653\n",
      "   -0.41945 ]\n",
      "  [ 4.607214  3.535342  5.358871  8.928098 11.408762  8.852825  2.042906\n",
      "   -3.304407]\n",
      "  [ 7.449527  8.639919 10.14406  10.064132  7.692013  1.909237 -3.412886\n",
      "   -4.26109 ]]]\n"
     ]
    }
   ],
   "source": [
    "# 원래 입력 / 가중치 / 바이어스 출력\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(precision=6)\n",
    "\n",
    "print(\"Original Input\")\n",
    "print(model.mp1_out_np[0])\n",
    "orig_input_1 = model.mp1_out_np[0][0]\n",
    "orig_input_2 = model.mp1_out_np[0][1]\n",
    "orig_input_3 = model.mp1_out_np[0][2]\n",
    "\n",
    "print(\"Original Weight\")\n",
    "print(model.conv2.weight.detach().numpy())\n",
    "orig_weight = model.conv2.weight.detach().numpy()\n",
    "\n",
    "print(\"Original Bias\")\n",
    "print(model.conv2.bias.detach().numpy())\n",
    "orig_bias = model.conv2.bias.detach().numpy()\n",
    "\n",
    "orig_output_calc_1 = np.zeros((3,8,8))\n",
    "orig_output_calc_2 = np.zeros((3,8,8))\n",
    "orig_output_calc_3 = np.zeros((3,8,8))\n",
    "orig_output_calc = np.zeros((3,8,8))\n",
    "\n",
    "for c in range(3):\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            orig_output_calc_1[c][i][j] += (orig_input_1[i:i+5, j:j+5] * orig_weight[c][0]).sum()\n",
    "            orig_output_calc_2[c][i][j] += (orig_input_2[i:i+5, j:j+5] * orig_weight[c][1]).sum()\n",
    "            orig_output_calc_3[c][i][j] += (orig_input_3[i:i+5, j:j+5] * orig_weight[c][2]).sum()         \n",
    "            orig_output_calc[c][i][j] = orig_output_calc_1[c][i][j] + orig_output_calc_2[c][i][j] + orig_output_calc_3[c][i][j] + orig_bias[c]\n",
    "        \n",
    "print(\"\\nBias : \")\n",
    "print(orig_bias)\n",
    "print(\"\\nCalc Value :\\n\")\n",
    "print(orig_output_calc) \n",
    "print(\"\\nReal Value :\\n\")\n",
    "print(model.conv2_out_np[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution Input\n",
      "(1, 3, 12, 12)\n",
      "[[[  1.   1.   1.   1.  62. 312. 367. 102.   1.   1.   1.   1.]\n",
      "  [  1.   1.   1.  67. 294. 624. 575. 227.  35.   1.   1.   1.]\n",
      "  [  1.   1.  34. 274. 640. 694. 656. 476. 315.  50.   1.   1.]\n",
      "  [  1.   7. 157. 608. 683. 677. 551. 472. 480. 203.  19.   1.]\n",
      "  [  1.  11. 347. 669. 629. 443. 191. 353. 488. 451. 107.   1.]\n",
      "  [  1.  37. 443. 588. 402.  66.   0. 220. 601. 550. 144.   1.]\n",
      "  [  1.  64. 561. 590.  97.   0.  94. 386. 656. 592. 125.   4.]\n",
      "  [  1. 101. 581. 578. 145. 214. 419. 662. 662. 443.   6.   1.]\n",
      "  [  1.  88. 559. 599. 489. 591. 669. 677. 603. 136.   0.   1.]\n",
      "  [  1.  55. 352. 520. 611. 662. 654. 522. 219.   0.   1.   1.]\n",
      "  [  1.  14. 146. 262. 405. 433. 314. 129.   2.   0.   1.   1.]\n",
      "  [  1.   1.  19.  87. 114.  73.  31.   0.   1.   1.   1.   1.]]\n",
      "\n",
      " [[  0.   0.   0.   0.  69. 304. 365. 213.   0.   0.   0.   0.]\n",
      "  [  0.   0.   0.  57. 202. 348. 388. 300.  86.   0.   0.   0.]\n",
      "  [  0.   0.  28. 211. 289. 280. 314. 506. 415. 146.   0.   0.]\n",
      "  [  0.   0. 139. 216. 226. 212. 123. 347. 407. 336.  60.   0.]\n",
      "  [  0.   0. 150. 177. 108.   0.   0.   0. 233. 392. 255.   0.]\n",
      "  [  0.   7.  24.   0.   0.   0.   0.   0.   0. 289. 254.  34.]\n",
      "  [  0.  40.  95. 130. 126.   0.  89. 202. 210. 140. 143.  26.]\n",
      "  [  0.  16.  56. 186. 267. 232. 351. 321. 244.  42.   8.   0.]\n",
      "  [  0.   0.   0. 292. 552. 510. 485. 289. 110.   5.   0.   0.]\n",
      "  [  0.   0.   0. 124. 360. 331. 224.   7.   0.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      " [[  0.   0.   0.   0. 104. 238. 202.  89.   5.   0.   0.   0.]\n",
      "  [  0.   0.   0.  66. 211. 290. 303. 243.  59.   2.   0.   0.]\n",
      "  [  0.   0.  44. 216. 260. 349. 539. 512. 239.  65.   0.   0.]\n",
      "  [  0.   7. 147. 227. 320. 356. 418. 428. 394. 250.  30.   0.]\n",
      "  [  0.  16. 166. 228. 277. 266. 196. 320. 437. 388. 141.   2.]\n",
      "  [  0.  34. 158. 193. 270. 213. 141.  78. 346. 421. 259.  13.]\n",
      "  [  0.  87. 174. 250. 304. 117.  99. 209. 276. 357. 276.  39.]\n",
      "  [  0.  74. 192. 355. 352. 234. 275. 266. 240. 279. 235.  45.]\n",
      "  [  0.  21. 180. 518. 496. 314. 318. 292. 262. 274. 155.   6.]\n",
      "  [  0.   0. 103. 438. 457. 369. 325. 231. 235. 214.  39.   0.]\n",
      "  [  0.   0.   0. 104. 236. 226. 212. 206. 176.  81.   0.   0.]\n",
      "  [  0.   0.   0.   0.  61. 121. 152. 120.  28.   0.   0.   0.]]]\n",
      "\n",
      "\n",
      "Convolution Weight\n",
      "(3, 3, 5, 5)\n",
      "[[[[ 21.  60.   3.  -5. -24.]\n",
      "   [ 28.  49.  -5.   2.   6.]\n",
      "   [ 21.  26.  -2.  26.   9.]\n",
      "   [ -2.   1. -29. -24. -21.]\n",
      "   [-37. -48. -58. -24. -36.]]\n",
      "\n",
      "  [[-35. -31.   5.  -9.  -8.]\n",
      "   [-47. -23.  11.  19.  35.]\n",
      "   [-63.  -7.  73.  44.  37.]\n",
      "   [-31.  30.  49.  14.   3.]\n",
      "   [-37.  21.  31.   4.  -3.]]\n",
      "\n",
      "  [[ 22.  22.  10.  -9. -11.]\n",
      "   [  5.  20.  47.  34.  38.]\n",
      "   [ 17.  15.  63.  40.  25.]\n",
      "   [  6.  25.  16.  10. -11.]\n",
      "   [-18. -22.  -1.   5. -28.]]]\n",
      "\n",
      "\n",
      " [[[  5.  12.   4. -10.   4.]\n",
      "   [-16. -21.  -6.   0.   6.]\n",
      "   [  3. -32. -52. -30. -16.]\n",
      "   [ 12.  24.   2. -16. -41.]\n",
      "   [-20.  17.  11.  19.   8.]]\n",
      "\n",
      "  [[-27. -13.  -7.  -9. -19.]\n",
      "   [ -3.  -1.   1.   5.  16.]\n",
      "   [ 16.   7. -17.  -5.  -9.]\n",
      "   [ 24.  57.  65.  28.  13.]\n",
      "   [-14.  18.  61.  60.  87.]]\n",
      "\n",
      "  [[-15.  -5.  13.   6.   5.]\n",
      "   [-11. -18.   8.   5.   5.]\n",
      "   [-12. -21. -22. -20. -19.]\n",
      "   [ 33.  29.  17. -18.   3.]\n",
      "   [  4.  18.  18.  28.  26.]]]\n",
      "\n",
      "\n",
      " [[[  3.  -6.  -6. -17.  19.]\n",
      "   [-31. -52. -19.  40.  66.]\n",
      "   [-25.  40.  59.  52.  23.]\n",
      "   [ 16.  19.  -9. -27.  -1.]\n",
      "   [  4. -25. -65. -44. -35.]]\n",
      "\n",
      "  [[ -5.   1. -22. -30.  -8.]\n",
      "   [  2.   2.  23.  24.  31.]\n",
      "   [ 13.  36.  55.  14.  10.]\n",
      "   [  9.  -1.  17.  -4.  14.]\n",
      "   [-13.  13.  33.  26.  12.]]\n",
      "\n",
      "  [[  2. -15. -12.   6.  20.]\n",
      "   [-28.   9.  29.  36.  16.]\n",
      "   [-10.  19.  -9. -27. -13.]\n",
      "   [-16. -16.  -8.  28.  13.]\n",
      "   [-22. -17.   7.   4. -11.]]]]\n",
      "Convolution Bias\n",
      "(3,)\n",
      "[-4. -6.  3.]\n",
      "Convolution Output\n",
      "(1, 3, 8, 8)\n",
      "[[[[ -106.    90.   250.   563.   928.   929.   377.  -251.]\n",
      "   [   95.   238.   569.  1235.  1621.  1524.   876.   198.]\n",
      "   [   15.   207.   578.  1010.  1288.  1374.  1062.   633.]\n",
      "   [ -210.    73.   534.   561.   502.   590.   786.   868.]\n",
      "   [ -267.   171.   595.   195.   -97.    56.   464.   932.]\n",
      "   [   15.   670.  1154.   609.    61.    40.   396.  1010.]\n",
      "   [  621.  1616.  2085.  1190.   551.   492.   743.  1121.]\n",
      "   [  891.  1848.  2162.  1436.   954.   903.  1092.  1141.]]\n",
      "\n",
      "  [[  147.    75.  -150.  -377.  -206.   314.   930.  1112.]\n",
      "   [ -235.  -294.  -434.  -671.  -781.  -492.    57.   635.]\n",
      "   [ -132.  -258.  -328.  -362.  -310.  -373.  -284.   -93.]\n",
      "   [  220.   374.   522.   452.   378.   128.    -8.  -228.]\n",
      "   [  398.   921.  1310.  1186.   798.   411.   135.  -143.]\n",
      "   [  -34.   426.   980.   984.   575.   167.   -74.  -113.]\n",
      "   [ -749.  -695.  -445.  -217.  -185.  -292.  -411.  -273.]\n",
      "   [ -589.  -831. -1024.  -999.  -812.  -630.  -460.  -252.]]\n",
      "\n",
      "  [[  -15.   529.   974.   750.   218.    13.   -42.   -64.]\n",
      "   [  504.  1033.  1236.   980.   516.    22.  -228.  -163.]\n",
      "   [  790.  1052.   921.   375.  -123.  -376.  -321.  -125.]\n",
      "   [  577.   489.   -57.  -509.  -465.  -203.   -74.    -4.]\n",
      "   [  347.    27.  -514.  -690.  -183.   439.   406.   153.]\n",
      "   [  407.   -99.  -394.  -104.   776.  1159.   736.   -54.]\n",
      "   [  590.   453.   686.  1143.  1460.  1133.   261.  -423.]\n",
      "   [  954.  1106.  1298.  1288.   985.   244.  -437.  -545.]]]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=0)\n",
    "\n",
    "print(\"Convolution Input\")\n",
    "print(np.shape(model.mp1_out_np))\n",
    "_input = model.mp1_out_np[0] * 128\n",
    "print(_input)\n",
    "print('\\n')\n",
    "\n",
    "print(\"Convolution Weight\")\n",
    "print(np.shape(model.conv2.weight.detach().numpy()))\n",
    "weight = model.conv2.weight.detach().numpy() * 128\n",
    "print(weight)\n",
    "\n",
    "print(\"Convolution Bias\")\n",
    "print(np.shape(model.conv2.bias.detach().numpy()))\n",
    "bias = model.conv2.bias.detach().numpy() * 128\n",
    "print(bias)\n",
    "\n",
    "print(\"Convolution Output\")\n",
    "print(np.shape(model.conv2_out_np))\n",
    "output = model.conv2_out_np * 128\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 12, 12)\n",
      "(3, 3, 5, 5)\n",
      "(1, 3, 8, 8)\n",
      "\n",
      "Bias : \n",
      "[-4. -6.  3.]\n",
      "\n",
      "Calc Value :\n",
      "\n",
      "[[[ -106.    90.   250.   563.   928.   929.   377.  -251.]\n",
      "  [   95.   238.   569.  1235.  1621.  1524.   876.   198.]\n",
      "  [   15.   207.   578.  1010.  1288.  1374.  1062.   633.]\n",
      "  [ -210.    73.   534.   561.   502.   590.   786.   868.]\n",
      "  [ -267.   171.   595.   195.   -97.    56.   464.   932.]\n",
      "  [   15.   670.  1154.   609.    61.    40.   396.  1010.]\n",
      "  [  621.  1616.  2085.  1190.   551.   492.   743.  1121.]\n",
      "  [  891.  1848.  2162.  1436.   954.   903.  1092.  1141.]]\n",
      "\n",
      " [[  147.    75.  -150.  -377.  -206.   314.   930.  1112.]\n",
      "  [ -235.  -294.  -434.  -671.  -781.  -492.    57.   635.]\n",
      "  [ -132.  -258.  -328.  -362.  -310.  -373.  -284.   -93.]\n",
      "  [  220.   374.   522.   452.   378.   128.    -8.  -228.]\n",
      "  [  398.   921.  1310.  1186.   798.   411.   135.  -143.]\n",
      "  [  -34.   426.   980.   984.   575.   167.   -74.  -113.]\n",
      "  [ -749.  -695.  -445.  -217.  -185.  -292.  -411.  -273.]\n",
      "  [ -589.  -831. -1024.  -999.  -812.  -630.  -460.  -252.]]\n",
      "\n",
      " [[  -15.   529.   974.   750.   218.    13.   -42.   -64.]\n",
      "  [  504.  1033.  1236.   980.   516.    22.  -228.  -163.]\n",
      "  [  790.  1052.   921.   375.  -123.  -376.  -321.  -125.]\n",
      "  [  577.   489.   -57.  -509.  -465.  -203.   -74.    -4.]\n",
      "  [  347.    27.  -514.  -690.  -183.   439.   406.   153.]\n",
      "  [  407.   -99.  -394.  -104.   776.  1159.   736.   -54.]\n",
      "  [  590.   453.   686.  1143.  1460.  1133.   261.  -423.]\n",
      "  [  954.  1106.  1298.  1288.   985.   244.  -437.  -545.]]]\n",
      "\n",
      "Real Value :\n",
      "\n",
      "[[[[ -106.    90.   250.   563.   928.   929.   377.  -251.]\n",
      "   [   95.   238.   569.  1235.  1621.  1524.   876.   198.]\n",
      "   [   15.   207.   578.  1010.  1288.  1374.  1062.   633.]\n",
      "   [ -210.    73.   534.   561.   502.   590.   786.   868.]\n",
      "   [ -267.   171.   595.   195.   -97.    56.   464.   932.]\n",
      "   [   15.   670.  1154.   609.    61.    40.   396.  1010.]\n",
      "   [  621.  1616.  2085.  1190.   551.   492.   743.  1121.]\n",
      "   [  891.  1848.  2162.  1436.   954.   903.  1092.  1141.]]\n",
      "\n",
      "  [[  147.    75.  -150.  -377.  -206.   314.   930.  1112.]\n",
      "   [ -235.  -294.  -434.  -671.  -781.  -492.    57.   635.]\n",
      "   [ -132.  -258.  -328.  -362.  -310.  -373.  -284.   -93.]\n",
      "   [  220.   374.   522.   452.   378.   128.    -8.  -228.]\n",
      "   [  398.   921.  1310.  1186.   798.   411.   135.  -143.]\n",
      "   [  -34.   426.   980.   984.   575.   167.   -74.  -113.]\n",
      "   [ -749.  -695.  -445.  -217.  -185.  -292.  -411.  -273.]\n",
      "   [ -589.  -831. -1024.  -999.  -812.  -630.  -460.  -252.]]\n",
      "\n",
      "  [[  -15.   529.   974.   750.   218.    13.   -42.   -64.]\n",
      "   [  504.  1033.  1236.   980.   516.    22.  -228.  -163.]\n",
      "   [  790.  1052.   921.   375.  -123.  -376.  -321.  -125.]\n",
      "   [  577.   489.   -57.  -509.  -465.  -203.   -74.    -4.]\n",
      "   [  347.    27.  -514.  -690.  -183.   439.   406.   153.]\n",
      "   [  407.   -99.  -394.  -104.   776.  1159.   736.   -54.]\n",
      "   [  590.   453.   686.  1143.  1460.  1133.   261.  -423.]\n",
      "   [  954.  1106.  1298.  1288.   985.   244.  -437.  -545.]]]]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(_input))\n",
    "print(np.shape(weight))\n",
    "print(np.shape(output))\n",
    "\n",
    "_input_1 = _input[0]\n",
    "_input_2 = _input[1]\n",
    "_input_3 = _input[2]\n",
    "\n",
    "output_calc_1 = np.zeros((3,8,8))\n",
    "output_calc_2 = np.zeros((3,8,8))\n",
    "output_calc_3 = np.zeros((3,8,8))\n",
    "output_calc = np.zeros((3,8,8))\n",
    "\n",
    "for c in range(3):\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            output_calc_1[c][i][j] += (_input_1[i:i+5, j:j+5] * weight[c][0]).sum()\n",
    "            output_calc_2[c][i][j] += (_input_2[i:i+5, j:j+5] * weight[c][1]).sum()\n",
    "            output_calc_3[c][i][j] += (_input_3[i:i+5, j:j+5] * weight[c][2]).sum()\n",
    "            output_calc[c][i][j] = output_calc_1[c][i][j] + output_calc_2[c][i][j] + output_calc_3[c][i][j] + bias[c] * 128\n",
    "        \n",
    "print(\"\\nBias : \")\n",
    "print(bias)\n",
    "print(\"\\nCalc Value :\\n\")\n",
    "print(output_calc / 128) \n",
    "print(\"\\nReal Value :\\n\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calc Value 1 :\n",
      "\n",
      "[[[ -79563. -115480. -119358.  -83185.  -17614.    8089.  -19178.\n",
      "    -43569.]\n",
      "  [ -76744.  -95861.  -57902.   21259.   71909.   58105.    3008.\n",
      "    -30303.]\n",
      "  [ -77871.  -72264.    -109.   69870.   77375.   31645.   -8140.\n",
      "    -15770.]\n",
      "  [ -86202.  -63898.    8516.   29483.      24.  -51432.  -51033.\n",
      "     -8752.]\n",
      "  [ -99805.  -74930.  -25465.  -50144. -102209. -119513.  -67308.\n",
      "     22131.]\n",
      "  [ -88203.  -55248.  -29550.  -82149. -126760. -100607.  -18583.\n",
      "     77434.]\n",
      "  [ -40519.    7861.   28970.  -29656.  -44251.    1708.   71418.\n",
      "    120942.]\n",
      "  [    207.   57557.   83441.   56935.   63897.   92173.  121985.\n",
      "    118717.]]\n",
      "\n",
      " [[ -34046.  -46650.  -68184.  -87613.  -83399.  -52588.   -9879.\n",
      "     10516.]\n",
      "  [ -49032.  -62741.  -77445.  -91922.  -91984.  -72616.  -38571.\n",
      "     -7807.]\n",
      "  [ -50142.  -56353.  -62881.  -67248.  -65087.  -66195.  -46464.\n",
      "    -24871.]\n",
      "  [ -39451.  -32929.  -31157.  -33193.  -32252.  -46073.  -45074.\n",
      "    -38079.]\n",
      "  [ -42196.  -27861.  -15240.  -17120.  -28000.  -43565.  -47254.\n",
      "    -41824.]\n",
      "  [ -65579.  -56775.  -36730.  -31698.  -49697.  -59595.  -57982.\n",
      "    -39881.]\n",
      "  [ -85973.  -90344.  -82505.  -75706.  -74452.  -69561.  -59637.\n",
      "    -30901.]\n",
      "  [ -66662.  -86434.  -94062.  -89457.  -77806.  -62378.  -39476.\n",
      "    -14770.]]\n",
      "\n",
      " [[ -38486.    6397.   60835.   45844.  -11971.  -58632.  -61148.\n",
      "    -35397.]\n",
      "  [  22569.   78118.  108770.   77235.   16527.  -54760.  -84750.\n",
      "    -59476.]\n",
      "  [  56531.   84800.   82467.   27848.  -25870.  -76233.  -79268.\n",
      "    -53509.]\n",
      "  [  39395.   31797.  -22594.  -75958.  -80973.  -51494.  -30267.\n",
      "    -18183.]\n",
      "  [   5579.  -54266. -123945. -127460.  -51183.   23978.   25844.\n",
      "      6909.]\n",
      "  [  -5916.  -82415. -115566.  -58365.   48530.   89494.   52990.\n",
      "     -6654.]\n",
      "  [  31453.   -5761.    -686.   68322.  119041.   96222.   14028.\n",
      "    -43055.]\n",
      "  [  76656.   76675.   87689.  108970.   93397.   21664.  -50750.\n",
      "    -61677.]]]\n",
      "\n",
      "Calc Value 2 :\n",
      "\n",
      "[[[ 45023.  79466.  75835.  55328.  30290.  19699.   9228. -10690.]\n",
      "  [ 51323.  60357.  31731.  13960.   8799.  19032.  12803.  -2882.]\n",
      "  [ 37552.  28601. -12539. -34950. -18572.  22370.  28756.   6759.]\n",
      "  [ 17666.   9621.  -7583. -20525.  -6994.  38394.  41780.  17059.]\n",
      "  [ 21433.  30710.  39205.  29735.  36684.  56781.  32740.  -3671.]\n",
      "  [ 33873.  59163.  92925.  84741.  63609.  32811. -16758. -34334.]\n",
      "  [ 46021.  84699. 110469.  68047.  16957. -26652. -62165. -53548.]\n",
      "  [ 39617.  63118.  62979.   2968. -51294. -71948. -64274. -35421.]]\n",
      "\n",
      " [[ 45343.  43920.  36116.  29940.  46025.  77925. 106012. 100425.]\n",
      "  [ 13476.  14788.  11718.   1124.  -5787.  16354.  48718.  73157.]\n",
      "  [ 20053.  10483.   7578.   8669.  15039.  22920.  22102.  19288.]\n",
      "  [ 47273.  58394.  70044.  62349.  62499.  51914.  39476.  10211.]\n",
      "  [ 71224. 116078. 145006. 134875. 103137.  70052.  41888.  17925.]\n",
      "  [ 52544.  93379. 130734. 127313.  98415.  58462.  30995.  15071.]\n",
      "  [  2063.  10601.  27907.  42373.  38786.  21170.   2458.  -6607.]\n",
      "  [ -1064.  -5235. -13180. -15772. -14474. -10434. -13943. -13238.]]\n",
      "\n",
      " [[ 30168.  48463.  52512.  54781.  57310.  76070.  76952.  59881.]\n",
      "  [ 28763.  35622.  34823.  38483.  45248.  60245.  69512.  64325.]\n",
      "  [ 27253.  27004.  18073.  10299.  11339.  29659.  42738.  52022.]\n",
      "  [ 14474.  13448.  15707.  13857.  17611.  21003.  24066.  28996.]\n",
      "  [ 19173.  41596.  60939.  53377.  43586.  36552.  27374.  18796.]\n",
      "  [ 31922.  52308.  73816.  74540.  71766.  56207.  35569.  11771.]\n",
      "  [ 21516.  45703.  80213.  86904.  78031.  48213.  22743.   1928.]\n",
      "  [ 20938.  37446.  56443.  51550.  32521.   8733.   -783.  -1501.]]]\n",
      "\n",
      "Calc Value 3 :\n",
      "\n",
      "[[[ 21507.  48105.  76038. 100458. 106709.  91610.  58685.  22734.]\n",
      "  [ 38109.  66452.  99575. 123376. 127301. 118519.  96831.  59132.]\n",
      "  [ 42826.  70663.  87155.  94943. 106571. 122408. 115910.  90594.]\n",
      "  [ 42183.  64189.  67970.  63348.  71739.  89112. 110431. 103391.]\n",
      "  [ 44789.  66599.  63024.  45951.  53598.  70426.  94459. 101381.]\n",
      "  [ 56780.  82393.  84836.  75853.  71508.  73472.  86565.  86780.]\n",
      "  [ 74571. 114790. 127931. 114487.  98364.  88482.  86380.  76636.]\n",
      "  [ 74820. 116464. 130873. 124393. 110091.  95862.  82553.  63306.]]\n",
      "\n",
      " [[  8238.  13086.  13667.  10215.  11733.  15620.  23614.  32204.]\n",
      "  [  6256.  11094.  10939.   5610.  -1428.  -5948.  -2150.  16718.]\n",
      "  [ 13910.  13578.  14042.  13058.  11116.  -3707. -11193.  -5508.]\n",
      "  [ 21146.  23138.  28682.  29481.  18934.  11291.   5357.   -563.]\n",
      "  [ 22713.  30438.  38672.  34787.  27822.  26919.  23405.   6338.]\n",
      "  [  9441.  18710.  32233.  31131.  25603.  23285.  18215.  11062.]\n",
      "  [-11157.  -8401.  -1606.   6285.  12740.  11731.   5276.   3313.]\n",
      "  [ -6885. -14005. -23032. -21893. -10859.  -7064.  -4747.  -3483.]]\n",
      "\n",
      " [[  6020.  12517.  10996.  -4925. -17731. -16174. -21517. -33084.]\n",
      "  [ 12876.  18157.  14242.   9371.   3979.  -2993. -14334. -26072.]\n",
      "  [ 16936.  22524.  17013.   9557.  -1557.  -1948.  -4940. -14813.]\n",
      "  [ 19580.  16972.   -822.  -3374.   3436.   4133.  -3674. -11679.]\n",
      "  [ 19354.  15711.  -3102. -14585. -16193.  -4689.  -1656.  -6511.]\n",
      "  [ 25738.  17053.  -9009. -29900. -21370.   2273.   5260. -12340.]\n",
      "  [ 22163.  17630.   7921.  -9300. -10502.    258.  -3651. -13363.]\n",
      "  [ 24108.  27084.  21717.   4019.   -243.    533.  -4735.  -6987.]]]\n",
      "Sum\n",
      "[[[ -13034.   12091.   32515.   72600.  119384.  119397.   48735.\n",
      "    -31524.]\n",
      "  [  12688.   30948.   73404.  158595.  208010.  195656.  112642.\n",
      "     25947.]\n",
      "  [   2508.   27000.   74506.  129864.  165374.  176423.  136526.\n",
      "     81583.]\n",
      "  [ -26353.    9912.   68902.   72306.   64769.   76074.  101178.\n",
      "    111698.]\n",
      "  [ -33583.   22379.   76764.   25543.  -11927.    7694.   59892.\n",
      "    119841.]\n",
      "  [   2451.   86309.  148211.   78445.    8357.    5677.   51223.\n",
      "    129880.]\n",
      "  [  80073.  207350.  267370.  152878.   71069.   63537.   95633.\n",
      "    144031.]\n",
      "  [ 114643.  237139.  277293.  184296.  122695.  116087.  140264.\n",
      "    146602.]]\n",
      "\n",
      " [[  19535.   10357.  -18402.  -47458.  -25641.   40957.  119747.\n",
      "    143145.]\n",
      "  [ -29301.  -36859.  -54788.  -85188.  -99198.  -62210.    7997.\n",
      "     82068.]\n",
      "  [ -16178.  -32291.  -41261.  -45521.  -38932.  -46982.  -35554.\n",
      "    -11091.]\n",
      "  [  28968.   48603.   67569.   58637.   49181.   17132.    -241.\n",
      "    -28432.]\n",
      "  [  51741.  118655.  168437.  152542.  102958.   53407.   18039.\n",
      "    -17561.]\n",
      "  [  -3594.   55314.  126237.  126747.   74322.   22152.   -8772.\n",
      "    -13748.]\n",
      "  [ -95066.  -88144.  -56204.  -27049.  -22926.  -36660.  -51904.\n",
      "    -34196.]\n",
      "  [ -74611. -105675. -130275. -127122. -103140.  -79876.  -58165.\n",
      "    -31491.]]\n",
      "\n",
      " [[  -2298.   67377.  124343.   95700.   27608.    1264.   -5713.\n",
      "     -8599.]\n",
      "  [  64208.  131897.  157834.  125089.   65755.    2492.  -29572.\n",
      "    -21223.]\n",
      "  [ 100719.  134328.  117553.   47704.  -16088.  -48523.  -41469.\n",
      "    -16300.]\n",
      "  [  73448.   62218.   -7709.  -65476.  -59926.  -26358.   -9876.\n",
      "      -866.]\n",
      "  [  44107.    3042.  -66109.  -88667.  -23789.   55841.   51561.\n",
      "     19194.]\n",
      "  [  51745.  -13054.  -50759.  -13725.   98927.  147974.   93818.\n",
      "     -7224.]\n",
      "  [  75133.   57572.   87448.  145926.  186570.  144693.   33119.\n",
      "    -54491.]\n",
      "  [ 121702.  141205.  165849.  164539.  125674.   30929.  -56268.\n",
      "    -70165.]]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=0)\n",
    "print(\"\\nCalc Value 1 :\\n\")\n",
    "print(output_calc_1) \n",
    "\n",
    "print(\"\\nCalc Value 2 :\\n\")\n",
    "print(output_calc_2) \n",
    "\n",
    "print(\"\\nCalc Value 3 :\\n\")\n",
    "print(output_calc_3) \n",
    "\n",
    "print(\"Sum\")\n",
    "print(output_calc_1 + output_calc_2 + output_calc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
